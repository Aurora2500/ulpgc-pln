{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El mecanismo de auto-atenci√≥n con enmascaramiento del modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoAtencionConEnmasc:\n",
    "    def score(self, Q: torch.Tensor, K: torch.Tensor):\n",
    "        return torch.matmul(Q, K.transpose(0, 1))\n",
    "\n",
    "    def scale(self, score: torch.Tensor, size):\n",
    "        return score.div(math.sqrt(size))\n",
    "\n",
    "    def mask(self, scaled: torch.Tensor):\n",
    "        mask = torch.ones_like(scaled).tril()\n",
    "        return scaled * mask\n",
    "\n",
    "    def softmax(self, scores: torch.Tensor):\n",
    "        inf_mask = torch.full_like(scores, -math.inf).triu(diagonal=1)\n",
    "        pre_soft = scores + inf_mask\n",
    "        function = nn.Softmax(dim=1)\n",
    "        post_soft = function(pre_soft)\n",
    "        return post_soft\n",
    "\n",
    "    def multiply_by_values(self, scores: torch.Tensor, V):\n",
    "        return torch.matmul(scores, V)\n",
    "\n",
    "    def calculate(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        scored = torch.matmul(Q, K.transpose(0, 1))\n",
    "        scaled = scored.div(math.sqrt(K.size()[1]))\n",
    "        masked = scaled * torch.ones_like(scaled).tril()\n",
    "        inf_mask = torch.full_like(masked, -math.inf).triu(diagonal=1)\n",
    "        pre_soft = masked + inf_mask\n",
    "        function = nn.Softmax(dim=1)\n",
    "        post_soft = function(pre_soft)\n",
    "        return torch.matmul(post_soft, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3000, 0.6000, 0.9000, 1.2000],\n",
       "        [0.0600, 0.1200, 0.1800, 0.2400],\n",
       "        [0.0900, 0.1800, 0.2700, 0.3600]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = AutoAtencionConEnmasc()\n",
    "\n",
    "score = transformer.score(Q, K)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1732, 0.3464, 0.5196, 0.6928],\n",
       "        [0.0346, 0.0693, 0.1039, 0.1386],\n",
       "        [0.0520, 0.1039, 0.1559, 0.2078]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = transformer.scale(score, K.size()[1])\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1732, 0.3464, 0.0000, 0.0000],\n",
       "        [0.0346, 0.0693, 0.1039, 0.0000],\n",
       "        [0.0520, 0.1039, 0.1559, 0.2078]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = transformer.mask(scaled)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full_like(masked, -math.inf).triu(diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4568, 0.5432, 0.0000, 0.0000],\n",
       "        [0.3219, 0.3332, 0.3449, 0.0000],\n",
       "        [0.2309, 0.2432, 0.2561, 0.2698]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed = transformer.softmax(masked)\n",
    "softmaxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.4568, 0.5432, 0.0000],\n",
       "        [0.3219, 0.3332, 0.3449],\n",
       "        [0.2309, 0.5130, 0.5260]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplied = transformer.multiply_by_values(softmaxed, V)\n",
    "multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.4568, 0.5432, 0.0000],\n",
       "        [0.3219, 0.3332, 0.3449],\n",
       "        [0.2309, 0.5130, 0.5260]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.calculate(Q, K, V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
